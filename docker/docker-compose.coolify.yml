name: anythingllm-coolify

networks:
  anything-llm:
    driver: bridge

volumes:
  anythingllm_storage:
    driver: local
  anythingllm_hotdir:
    driver: local
  anythingllm_outputs:
    driver: local

services:
  anything-llm:
    container_name: anythingllm
    build:
      context: .
      dockerfile: ./Dockerfile
    # Removed cap_add SYS_ADMIN - not needed and causes issues with Coolify
    volumes:
      # Using named volumes instead of host paths for Coolify compatibility
      - anythingllm_storage:/app/server/storage
      - anythingllm_hotdir:/app/collector/hotdir
      - anythingllm_outputs:/app/collector/outputs
    # Removed user directive - let Coolify handle user context
    ports:
      - "3001:3001"
    environment:
      # Core configuration
      - SERVER_PORT=3001
      - STORAGE_DIR=/app/server/storage
      
      # Add your LLM provider configuration here
      # Example for OpenAI:
      # - LLM_PROVIDER=openai
      # - OPEN_AI_KEY=${OPEN_AI_KEY}
      # - OPEN_MODEL_PREF=gpt-4o
      
      # Example for Ollama:
      # - LLM_PROVIDER=ollama
      # - OLLAMA_BASE_PATH=http://your-ollama-server:11434
      # - OLLAMA_MODEL_PREF=llama2
      
      # Vector database (default: lancedb)
      - VECTOR_DB=lancedb
      
      # Security tokens - MUST be set in Coolify environment
      - SIG_KEY=${SIG_KEY}
      - SIG_SALT=${SIG_SALT}
      - JWT_SECRET=${JWT_SECRET}
      
      # Optional: Authentication
      # - AUTH_TOKEN=${AUTH_TOKEN}
      
      # Disable telemetry by default for privacy
      - DISABLE_TELEMETRY=true
    networks:
      - anything-llm
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:3001/api/ping"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s